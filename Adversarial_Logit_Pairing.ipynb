{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JWLdMYfqYwz9","outputId":"407ec4b1-12ee-42d1-f54d-d35e6320bc4d","executionInfo":{"status":"ok","timestamp":1717765940617,"user_tz":-540,"elapsed":70957,"user":{"displayName":"Dayoon Lee","userId":"05021813261182781231"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.5/289.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchattacks\n","  Downloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (2.3.0+cu121)\n","Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (0.18.0+cu121)\n","Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.11.4)\n","Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (4.66.4)\n","Collecting requests~=2.25.1 (from torchattacks)\n","  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.25.2)\n","Collecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks)\n","  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests~=2.25.1->torchattacks)\n","  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (2024.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (4.12.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.7.1->torchattacks)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.1->torchattacks)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8.2->torchattacks) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->torchattacks) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->torchattacks) (1.3.0)\n","Installing collected packages: urllib3, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, idna, chardet, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchattacks\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.7\n","    Uninstalling urllib3-2.0.7:\n","      Successfully uninstalled urllib3-2.0.7\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.7\n","    Uninstalling idna-3.7:\n","      Successfully uninstalled idna-3.7\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 1.8.0 requires requests>=2.27.1, but you have requests 2.25.1 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.25.1 which is incompatible.\n","tweepy 4.14.0 requires requests<3,>=2.27.0, but you have requests 2.25.1 which is incompatible.\n","yfinance 0.2.40 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed chardet-4.0.0 idna-2.10 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 requests-2.25.1 torchattacks-3.5.1 urllib3-1.26.18\n"]}],"source":["!pip install wandb -qU\n","!pip install torchattacks"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x03Tgq-vzUSU","outputId":"14ccbf46-82ca-49ad-b3b0-870f6470c6c0","executionInfo":{"status":"ok","timestamp":1717765945832,"user_tz":-540,"elapsed":5220,"user":{"displayName":"Dayoon Lee","userId":"05021813261182781231"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["running on the GPU\n"]}],"source":["import numpy as np\n","import torch\n","import torchvision\n","import os\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torchvision.transforms import ToTensor\n","from torchvision.transforms.functional import to_pil_image\n","from torch.utils.data.dataset import random_split\n","from torchvision import models\n","from torch.utils.data import Dataset\n","from torchattacks import PGD\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","    print(\"running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"running on the CPU\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qFQ9LmWJa_j","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"d3801938-4b61-4981-a01e-f42867d87bf6","executionInfo":{"status":"ok","timestamp":1717765970353,"user_tz":-540,"elapsed":24527,"user":{"displayName":"Dayoon Lee","userId":"05021813261182781231"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"markdown","source":["# 1. Data Preprocessing"],"metadata":{"id":"DzASbDvH_00B"}},{"cell_type":"code","source":["#Hyperparams\n","batch_size = 64\n","learning_rate = 1e-3\n","epochs = 16"],"metadata":{"id":"aP1v2QwY4M5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_C-DTipnBqOq","outputId":"43787ce9-0e2a-4845-f221-bfc967a28468","executionInfo":{"status":"ok","timestamp":1717765994408,"user_tz":-540,"elapsed":17970,"user":{"displayName":"Dayoon Lee","userId":"05021813261182781231"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:12<00:00, 13241000.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","40000\n","10000\n"]}],"source":["train_transform = transforms.Compose([\n","        transforms.RandomCrop(32,padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","test_transform =  transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform = train_transform)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","\n","                                       download=True, transform = test_transform)\n","\n","torch.manual_seed(42)\n","train_len = len(trainset)\n","num_train = int(train_len * 0.8)\n","num_val = train_len - num_train\n","train_set,val_set = random_split(trainset,[num_train, num_val])\n","\n","trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n","                                              shuffle=True, num_workers=2)\n","valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n","                                            shuffle=False, num_workers=2)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n","print(len(train_set))\n","print(len(val_set))"]},{"cell_type":"markdown","source":["# 2. Models"],"metadata":{"id":"EMA-Ezf5_6C0"}},{"cell_type":"code","source":["#Resnet 50\n","class Bottleneck(nn.Module):\n","    def __init__(self, in_channel, mid_channel, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.out_channel = mid_channel * 4\n","        self.conv1 = nn.Conv2d(in_channel, mid_channel, 1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(mid_channel)\n","        self.conv2 = nn.Conv2d(mid_channel, mid_channel, 3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(mid_channel)\n","        self.conv3 = nn.Conv2d(mid_channel, self.out_channel, 1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.out_channel)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        if downsample or in_channel != self.out_channel or stride != 1:\n","            self.downsample_layers = nn.Sequential(\n","                nn.Conv2d(in_channel, self.out_channel, 1, stride=stride),\n","                nn.BatchNorm2d(self.out_channel)\n","            )\n","        else:\n","            self.downsample_layers = None\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample_layers is not None:\n","            identity = self.downsample_layers(identity)\n","\n","        out += identity\n","        out = self.relu(out)\n","        return out\n","\n","\n","\n","\n","class ResNet50(nn.Module):\n","  def __init__(self,num_classes=10):\n","    super(ResNet50,self).__init__()\n","    self.inplanes = 64\n","    self.block = Bottleneck\n","    self.AvgPool = nn.AvgPool2d(4)\n","\n","    self.conv1 = nn.Conv2d(3,self.inplanes,kernel_size=3,stride=1,padding=1,bias=False)\n","    self.bn = nn.BatchNorm2d(64)\n","    self.relu = nn.ReLU(inplace=True)\n","    ## conv1 통과후->32*32*64->spatial size 그대로!!\n","    self.layer1 = self._make_layer(64,num_blocks=3,stride=1)\n","\n","    #layer2 부터는 feature map의 spatial size도 절반씩 감소시켜줄거임!\n","    self.layer2 = self._make_layer(128,num_blocks=4,stride=2)\n","    self.layer3 = self._make_layer(256,num_blocks=6,stride=2)\n","    self.layer4 = self._make_layer(512,num_blocks=3,stride=2)\n","    #spatial size 작아서 굳이 pooling안해도 될듯?\n","    self.fc = nn.Linear(512*4,num_classes)\n","\n","  def _make_layer(self,planes,num_blocks,stride):\n","    layers = []\n","    #spatial size를 줄일때: layer의 첫번째 블록만 size 줄이면 됨!\n","    layers.append(Bottleneck(self.inplanes,planes,stride))\n","    self.inplanes = planes*4\n","    for i in range(num_blocks-1):\n","      layers.append(Bottleneck(self.inplanes,planes))\n","    return nn.Sequential(*layers)\n","\n","  def forward(self,x):\n","     out = self.conv1(x)\n","     out = self.bn(out)\n","     out = self.relu(out)\n","     out = self.layer1(out)\n","     #print(f'after layer1:{out.shape}')\n","     out = self.layer2(out)\n","     #print(f'after layer2:{out.shape}')\n","     out = self.layer3(out)\n","     #print(f'after layer3:{out.shape}')\n","     out = self.layer4(out)\n","     #print(f'after layer4:{out.shape}')\n","     out = self.AvgPool(out)\n","     out = out.view(out.size(0),-1)\n","     out = self.fc(out)\n","     #print(out.shape)\n","     #print(\"---------end-------------\")\n","     return out\n"],"metadata":{"id":"S-1q_tuDGyYy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#4. Training Methods"],"metadata":{"id":"shozObJkAFQd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIj-3mKF_dEO"},"outputs":[],"source":["# Directory to save adversarial examples\n","import torch.backends.cudnn as cudnn\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.deterministic = True\n","\n","\n","def vanilla_train1(dataloader, model, attack, loss_fn, optimizer,iam=None): #train on adv examples only\n","    model.train()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    train_loss, train_acc = 0, 0\n","    for batch, (X, y) in enumerate(dataloader):\n","        if(batch%20==0):\n","          print(f'batch no. {batch}')\n","        X = X.to(device)\n","        y = y.to(device)\n","        optimizer.zero_grad()\n","        adv = attack(X,y)\n","        pred_adv = model(adv) #prediction on adv example\n","        loss = loss_fn(pred_adv,y)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        train_acc += (pred_adv.argmax(1) == y).type(torch.float).sum().item()\n","\n","    train_loss /= num_batches #batch당 평균 train loss->estimate of train loss over entire dataset\n","    train_acc /= size\n","    print(f\"Training on Adv: \\n Accuracy: {(100*train_acc):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n","\n","    return train_loss, train_acc\n","\n","def vanilla_train2(dataloader, model, attack, loss_fn, optimizer,iam=None): #adv+benign 1:1\n","    model.train()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    train_loss, train_acc = 0, 0\n","    for batch, (X, y) in enumerate(dataloader):\n","        if(batch%20==0):\n","          print(f'batch no. {batch}')\n","        X = X.to(device)\n","        y = y.to(device)\n","        optimizer.zero_grad()\n","        adv = attack(X,y)\n","\n","        pred = model(X)#prediction on benign example\n","        pred_adv = model(adv)#prediction on adv example\n","        loss = torch.mean(loss_fn(pred_adv, y)+loss_fn(pred,y))\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        train_acc += (pred_adv.argmax(1) == y).type(torch.float).sum().item()\n","        train_acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    train_loss /= num_batches\n","    train_acc /= (2*size)\n","    print(f\"Train on Adv+Benign: \\n Accuracy: {(100*train_acc):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n","\n","    return train_loss, train_acc\n","\n","\n","def train_alp(dataloader, model, attack, loss_fn, optimizer, lam=0.1): #adv+benign 1:1\n","    model.train()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    train_loss, train_acc = 0, 0\n","    for batch, (X, y) in enumerate(dataloader):\n","        if(batch%20==0):\n","          print(f'batch no. {batch}')\n","        X = X.to(device)\n","        y = y.to(device)\n","        optimizer.zero_grad()\n","        adv = attack(X,y)\n","\n","        pred = model(X)#prediction on benign example\n","        pred_adv = model(adv)#prediction on adv example\n","        loss = torch.mean(loss_fn(pred_adv, y)+loss_fn(pred,y))+lam * torch.mean(torch.abs(pred - pred_adv))\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        train_acc += (pred_adv.argmax(1) == y).type(torch.float).sum().item()\n","        train_acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    train_loss /= num_batches\n","    train_acc /= (2*size)\n","    print(f\"ALP Train: \\n Accuracy: {(100*train_acc):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n","\n","    return train_loss, train_acc\n","\n","def validation_loop(dataloader, model, attack, loss_fn):\n","    model.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    total_loss,total_acc = 0, 0\n","    benign_loss, benign_acc = 0, 0\n","    adv_loss, adv_acc = 0, 0\n","    total = 0\n","    for batch,(X,y) in enumerate(dataloader):\n","      with torch.no_grad():\n","        X = X.to(device)\n","        y = y.to(device)\n","        total += y.size(0)\n","        pred = model(X)\n","        benign_loss += loss_fn(pred, y).item()\n","        benign_acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n","      X.requires_grad = True\n","      adv = attack(X,y)\n","      X.requires_grad = False\n","      with torch.no_grad():\n","        pred_adv = model(adv)\n","        adv_loss += loss_fn(pred_adv, y).item()\n","        adv_acc += (pred_adv.argmax(1) == y).type(torch.float).sum().item()\n","\n","    # Compute total loss and accuracy\n","    benign_loss /= num_batches\n","    benign_acc /= size\n","    adv_loss /= num_batches\n","    adv_acc /= size\n","    total_loss = (benign_loss + adv_loss) / 2\n","    total_acc = (benign_acc + adv_acc) / 2\n","    print('\\nTotal benign test accuarcy:', 100.*benign_acc)\n","    print('Total adversarial test Accuarcy:', 100.*adv_acc)\n","    print('Total benign test loss:', benign_loss)\n","    print('Total adversarial test loss:', adv_loss)\n","    print('**Summary**')\n","    print('Total validation accuracy:',100*total_acc)\n","    print('Total validiation loss:',total_loss)\n","\n","    return total_loss,total_acc,benign_loss,benign_acc,adv_loss,adv_acc"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"34PAOneUCeBo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3412ab19-f5be-4a57-bc36-f60babae4088","executionInfo":{"status":"ok","timestamp":1717766024972,"user_tz":-540,"elapsed":24920,"user":{"displayName":"Dayoon Lee","userId":"05021813261182781231"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_avJH3BUAnIl"},"outputs":[],"source":["def run(model,optimizer,method ='vanilla_train1',iam=0.1,model_save_name = None):\n","    # Initialize Weights & Biases\n","    wandb.init(\n","        project=\"First Trials\",\n","        name=\"experiment_first\",\n","        config={\n","            \"learning_rate\": learning_rate,\n","            \"batch_size\": batch_size,\n","            \"epochs\": epochs,\n","        })\n","\n","    # Define device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","   # Move model to device\n","    model = model.to(device)\n","    loss_fn = nn.CrossEntropyLoss()\n","    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","    attack = PGD(model, eps=8/255,alpha=2/255,steps=7)\n","    train = None\n","    if(method == 'vanilla_train1'):\n","      train = vanilla_train1\n","    elif(method == 'vanilla_train2'):\n","      train = vanilla_train2\n","    else:\n","      train = train_alp\n","    best_validation_acc = 0.0\n","    for t in range(epochs):\n","        print(f\"Epoch {t+1}\\n-------------------------------\")\n","        train_loss, train_acc = train(trainloader, model, attack,loss_fn, optimizer,iam)\n","        total_loss,total_acc,benign_loss,benign_acc,adv_loss,adv_acc = validation_loop(valloader, model, attack, loss_fn)\n","\n","        # sample_batch, _ = next(iter(trainloader))\n","        # sample_img = to_pil_image(sample_batch[0])\n","        # sample_img = wandb.Image(sample_img, caption=\"sample image\")\n","\n","        # images = [to_pil_image(image) for image in sample_batch[:5]]  # multiple images\n","\n","        wandb.log({\n","            \"train_acc\": train_acc,\n","            \"train_loss\": train_loss,\n","            \"benign_loss\":benign_loss,\n","            \"benign_acc\":benign_acc,\n","            \"adv_loss\":adv_loss,\n","            \"adv_acc\":adv_acc,\n","            \"total_acc\":total_acc,\n","            \"total_loss\":total_loss,\n","            # \"example\": sample_img,\n","            # \"examples\": [wandb.Image(image) for image in images],\n","        })\n","\n","        if total_acc > best_validation_acc:\n","            best_validation_acc = total_acc\n","            print(f\"Model at epoch{t} saved\")\n","            path = F\"/content/drive/MyDrive/AI_project/SaveModel/best_{model_save_name}\"\n","            torch.save(model.state_dict(), path)\n","        path = F\"/content/drive/MyDrive/AI_project/SaveModel/last_{model_save_name}\"\n","        torch.save(model.state_dict(), path)\n","\n","    print(\"Done!\")\n","    wandb.finish()"]},{"cell_type":"code","source":["#vanilla model2\n","torch.manual_seed(42)\n","vanilla_model2 = ResNet50().to(device)\n","optimizer = torch.optim.SGD(vanilla_model2.parameters(), lr=learning_rate, momentum=0.9)\n","#optimizer = torch.optim.Adam(vanilla_model2.parameters(), lr=learning_rate)\n","run(vanilla_model2,optimizer,'vanilla_train2',model_save_name = 'vanilla2.pt')"],"metadata":{"id":"ke6WVIu5cYFg","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["50295d36f123455bbdd1d0abfc6149e2","3dc4575271324d528878828d2993eb22","3aeda55ac16d44e3a5231ea3e930992d","137b9d438c27461e8be17b8835f9e252","b85a0cb9bd304557b73fb538ff953b2a","1d20e5bee8cd4ad8b8fe2c55a6163591","e52bd17b64234292b090d806e6df29bf","51d7de4c80d94c4980b53aab78633019"]},"outputId":"b7c7ba7c-745a-4215-fcfc-0dc8950e9c54","executionInfo":{"status":"ok","timestamp":1717768183483,"user_tz":-540,"elapsed":2116266,"user":{"displayName":"Dayoon Lee","userId":"05021813261182781231"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdayoonl055\u001b[0m (\u001b[33mdayoonlee\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.17.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240607_131427-k9vk9bfl</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/dayoonlee/First%20Trials/runs/k9vk9bfl' target=\"_blank\">experiment_first</a></strong> to <a href='https://wandb.ai/dayoonlee/First%20Trials' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/dayoonlee/First%20Trials' target=\"_blank\">https://wandb.ai/dayoonlee/First%20Trials</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/dayoonlee/First%20Trials/runs/k9vk9bfl' target=\"_blank\">https://wandb.ai/dayoonlee/First%20Trials/runs/k9vk9bfl</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 31.8%, Avg loss: 3.786319 \n","\n","\n","Total benign test accuarcy: 39.33\n","Total adversarial test Accuarcy: 9.19\n","Total benign test loss: 1.6504408705766034\n","Total adversarial test loss: 3.2044336461717156\n","**Summary**\n","Total validation accuracy: 24.259999999999998\n","Total validiation loss: 2.4274372583741597\n","Model at epoch0 saved\n","Epoch 2\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 45.9%, Avg loss: 2.998538 \n","\n","\n","Total benign test accuarcy: 43.47\n","Total adversarial test Accuarcy: 8.690000000000001\n","Total benign test loss: 1.5681713811910836\n","Total adversarial test loss: 3.804760554793534\n","**Summary**\n","Total validation accuracy: 26.08\n","Total validiation loss: 2.6864659679923086\n","Model at epoch1 saved\n","Epoch 3\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 52.7%, Avg loss: 2.637405 \n","\n","\n","Total benign test accuarcy: 47.18\n","Total adversarial test Accuarcy: 11.559999999999999\n","Total benign test loss: 1.4895155179272792\n","Total adversarial test loss: 3.7827733656403364\n","**Summary**\n","Total validation accuracy: 29.37\n","Total validiation loss: 2.6361444417838076\n","Model at epoch2 saved\n","Epoch 4\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 57.9%, Avg loss: 2.375693 \n","\n","\n","Total benign test accuarcy: 47.25\n","Total adversarial test Accuarcy: 9.049999999999999\n","Total benign test loss: 1.5210928051335038\n","Total adversarial test loss: 3.903497299570946\n","**Summary**\n","Total validation accuracy: 28.15\n","Total validiation loss: 2.712295052352225\n","Model at epoch3 saved\n","Epoch 5\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 61.8%, Avg loss: 2.156519 \n","\n","\n","Total benign test accuarcy: 45.34\n","Total adversarial test Accuarcy: 10.24\n","Total benign test loss: 1.5578470754015976\n","Total adversarial test loss: 3.650797379244665\n","**Summary**\n","Total validation accuracy: 27.790000000000003\n","Total validiation loss: 2.6043222273231312\n","Model at epoch4 saved\n","Epoch 6\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 64.8%, Avg loss: 1.995703 \n","\n","\n","Total benign test accuarcy: 50.61\n","Total adversarial test Accuarcy: 12.23\n","Total benign test loss: 1.4024879207276995\n","Total adversarial test loss: 3.4098709616691445\n","**Summary**\n","Total validation accuracy: 31.419999999999998\n","Total validiation loss: 2.406179441198422\n","Model at epoch5 saved\n","Epoch 7\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 67.5%, Avg loss: 1.837171 \n","\n","\n","Total benign test accuarcy: 55.13\n","Total adversarial test Accuarcy: 14.099999999999998\n","Total benign test loss: 1.2507416484462228\n","Total adversarial test loss: 3.8611574582992847\n","**Summary**\n","Total validation accuracy: 34.615\n","Total validiation loss: 2.555949553372754\n","Model at epoch6 saved\n","Epoch 8\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 70.2%, Avg loss: 1.690819 \n","\n","\n","Total benign test accuarcy: 56.14\n","Total adversarial test Accuarcy: 11.09\n","Total benign test loss: 1.245952555708065\n","Total adversarial test loss: 3.9012278189325027\n","**Summary**\n","Total validation accuracy: 33.615\n","Total validiation loss: 2.573590187320284\n","Model at epoch7 saved\n","Epoch 9\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 71.8%, Avg loss: 1.608901 \n","\n","\n","Total benign test accuarcy: 55.669999999999995\n","Total adversarial test Accuarcy: 13.08\n","Total benign test loss: 1.265657688781714\n","Total adversarial test loss: 3.979417764457168\n","**Summary**\n","Total validation accuracy: 34.375\n","Total validiation loss: 2.622537726619441\n","Model at epoch8 saved\n","Epoch 10\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 73.6%, Avg loss: 1.504939 \n","\n","\n","Total benign test accuarcy: 49.69\n","Total adversarial test Accuarcy: 14.249999999999998\n","Total benign test loss: 1.550231717954016\n","Total adversarial test loss: 3.9516005622353525\n","**Summary**\n","Total validation accuracy: 31.97\n","Total validiation loss: 2.7509161400946844\n","Model at epoch9 saved\n","Epoch 11\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 74.9%, Avg loss: 1.432237 \n","\n","\n","Total benign test accuarcy: 56.779999999999994\n","Total adversarial test Accuarcy: 12.57\n","Total benign test loss: 1.2418608570554455\n","Total adversarial test loss: 4.01176760302987\n","**Summary**\n","Total validation accuracy: 34.675\n","Total validiation loss: 2.626814230042658\n","Model at epoch10 saved\n","Epoch 12\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 76.2%, Avg loss: 1.361707 \n","\n","\n","Total benign test accuarcy: 59.89\n","Total adversarial test Accuarcy: 12.86\n","Total benign test loss: 1.1718956201699129\n","Total adversarial test loss: 4.180554675448472\n","**Summary**\n","Total validation accuracy: 36.375\n","Total validiation loss: 2.6762251478091925\n","Model at epoch11 saved\n","Epoch 13\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 77.2%, Avg loss: 1.306322 \n","\n","\n","Total benign test accuarcy: 57.43000000000001\n","Total adversarial test Accuarcy: 15.010000000000002\n","Total benign test loss: 1.3296363220852652\n","Total adversarial test loss: 3.692801153583891\n","**Summary**\n","Total validation accuracy: 36.22\n","Total validiation loss: 2.5112187378345783\n","Model at epoch12 saved\n","Epoch 14\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 78.2%, Avg loss: 1.240811 \n","\n","\n","Total benign test accuarcy: 59.08\n","Total adversarial test Accuarcy: 15.129999999999999\n","Total benign test loss: 1.1755732966076797\n","Total adversarial test loss: 3.49601645985986\n","**Summary**\n","Total validation accuracy: 37.105\n","Total validiation loss: 2.33579487823377\n","Model at epoch13 saved\n","Epoch 15\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 78.9%, Avg loss: 1.206020 \n","\n","\n","Total benign test accuarcy: 56.8\n","Total adversarial test Accuarcy: 14.57\n","Total benign test loss: 1.3129915750710068\n","Total adversarial test loss: 4.183451942577483\n","**Summary**\n","Total validation accuracy: 35.685\n","Total validiation loss: 2.7482217588242452\n","Model at epoch14 saved\n","Epoch 16\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","Train on Adv+Benign: \n"," Accuracy: 80.1%, Avg loss: 1.135470 \n","\n","\n","Total benign test accuarcy: 61.809999999999995\n","Total adversarial test Accuarcy: 15.47\n","Total benign test loss: 1.113898701348882\n","Total adversarial test loss: 3.8043613023818676\n","**Summary**\n","Total validation accuracy: 38.63999999999999\n","Total validiation loss: 2.4591300018653746\n","Model at epoch15 saved\n","Done!\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50295d36f123455bbdd1d0abfc6149e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>adv_acc</td><td>▂▁▄▁▃▅▇▃▆▇▅▅██▇█</td></tr><tr><td>adv_loss</td><td>▁▅▅▆▄▂▆▆▇▆▇█▄▃█▅</td></tr><tr><td>benign_acc</td><td>▁▂▃▃▃▅▆▆▆▄▆▇▇▇▆█</td></tr><tr><td>benign_loss</td><td>█▇▆▆▇▅▃▃▃▇▃▂▄▂▄▁</td></tr><tr><td>total_acc</td><td>▁▂▃▃▃▄▆▆▆▅▆▇▇▇▇█</td></tr><tr><td>total_loss</td><td>▃▇▆▇▆▂▅▅▆█▆▇▄▁█▃</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>adv_acc</td><td>0.1547</td></tr><tr><td>adv_loss</td><td>3.80436</td></tr><tr><td>benign_acc</td><td>0.6181</td></tr><tr><td>benign_loss</td><td>1.1139</td></tr><tr><td>total_acc</td><td>0.3864</td></tr><tr><td>total_loss</td><td>2.45913</td></tr><tr><td>train_acc</td><td>0.80068</td></tr><tr><td>train_loss</td><td>1.13547</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">experiment_first</strong> at: <a href='https://wandb.ai/dayoonlee/First%20Trials/runs/k9vk9bfl' target=\"_blank\">https://wandb.ai/dayoonlee/First%20Trials/runs/k9vk9bfl</a><br/> View project at: <a href='https://wandb.ai/dayoonlee/First%20Trials' target=\"_blank\">https://wandb.ai/dayoonlee/First%20Trials</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240607_131427-k9vk9bfl/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":["#alp model\n","torch.manual_seed(42)\n","alp_model = ResNet50().to(device)\n","optimizer = torch.optim.SGD(alp_model.parameters(), lr=learning_rate, momentum=0.9)\n","run(alp_model,optimizer,'alp_train',iam=0.1,model_save_name = 'alp.pt')"],"metadata":{"id":"lnIZ-M_jcYN0","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ec7d3245a0f54667bb4d964271751527","25c07fc9d34240588b693464e5f0a030","6ef1db049f804ae3bbd934771699e08c","bd61ca97f43440e5adbc6c17c521fafd","1b2c63e5dd7e4cd080cc2ea131601ed7","9f2b4799b55240078f0efe19570e5fa1","fd67e7a981f440c7b8eb6792c05691a6","2b2565744faf42cbbff83e3cf8a81dcf"]},"executionInfo":{"status":"ok","timestamp":1717771025702,"user_tz":-540,"elapsed":2128152,"user":{"displayName":"Dayoon Lee","userId":"05021813261182781231"}},"outputId":"8dcb3ab0-96fd-4959-cabb-9e5502831da7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.17.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240607_140137-etgnz4uc</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/dayoonlee/First%20Trials/runs/etgnz4uc' target=\"_blank\">experiment_first</a></strong> to <a href='https://wandb.ai/dayoonlee/First%20Trials' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/dayoonlee/First%20Trials' target=\"_blank\">https://wandb.ai/dayoonlee/First%20Trials</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/dayoonlee/First%20Trials/runs/etgnz4uc' target=\"_blank\">https://wandb.ai/dayoonlee/First%20Trials/runs/etgnz4uc</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 31.7%, Avg loss: 3.823396 \n","\n","\n","Total benign test accuarcy: 40.589999999999996\n","Total adversarial test Accuarcy: 10.95\n","Total benign test loss: 1.638281203379297\n","Total adversarial test loss: 3.059768385188595\n","**Summary**\n","Total validation accuracy: 25.77\n","Total validiation loss: 2.349024794283946\n","Model at epoch0 saved\n","Epoch 2\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 45.1%, Avg loss: 3.077237 \n","\n","\n","Total benign test accuarcy: 44.190000000000005\n","Total adversarial test Accuarcy: 8.93\n","Total benign test loss: 1.5479055567152182\n","Total adversarial test loss: 3.597404909741347\n","**Summary**\n","Total validation accuracy: 26.56\n","Total validiation loss: 2.5726552332282826\n","Model at epoch1 saved\n","Epoch 3\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 52.2%, Avg loss: 2.724682 \n","\n","\n","Total benign test accuarcy: 51.32\n","Total adversarial test Accuarcy: 10.22\n","Total benign test loss: 1.3639378752678064\n","Total adversarial test loss: 3.5876313926308017\n","**Summary**\n","Total validation accuracy: 30.769999999999996\n","Total validiation loss: 2.475784633949304\n","Model at epoch2 saved\n","Epoch 4\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 57.6%, Avg loss: 2.449870 \n","\n","\n","Total benign test accuarcy: 54.61\n","Total adversarial test Accuarcy: 8.66\n","Total benign test loss: 1.2731386578766404\n","Total adversarial test loss: 3.911234211769833\n","**Summary**\n","Total validation accuracy: 31.635\n","Total validiation loss: 2.592186434823237\n","Model at epoch3 saved\n","Epoch 5\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 61.8%, Avg loss: 2.223381 \n","\n","\n","Total benign test accuarcy: 53.900000000000006\n","Total adversarial test Accuarcy: 10.69\n","Total benign test loss: 1.2715010954316255\n","Total adversarial test loss: 3.561051710396056\n","**Summary**\n","Total validation accuracy: 32.295\n","Total validiation loss: 2.416276402913841\n","Model at epoch4 saved\n","Epoch 6\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 65.2%, Avg loss: 2.051964 \n","\n","\n","Total benign test accuarcy: 53.47\n","Total adversarial test Accuarcy: 11.65\n","Total benign test loss: 1.3376962291966579\n","Total adversarial test loss: 3.5774760565180688\n","**Summary**\n","Total validation accuracy: 32.56\n","Total validiation loss: 2.4575861428573633\n","Model at epoch5 saved\n","Epoch 7\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 67.9%, Avg loss: 1.894254 \n","\n","\n","Total benign test accuarcy: 57.550000000000004\n","Total adversarial test Accuarcy: 12.809999999999999\n","Total benign test loss: 1.2128111818793472\n","Total adversarial test loss: 3.306926087968668\n","**Summary**\n","Total validation accuracy: 35.18\n","Total validiation loss: 2.2598686349240076\n","Model at epoch6 saved\n","Epoch 8\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 70.3%, Avg loss: 1.770881 \n","\n","\n","Total benign test accuarcy: 61.23\n","Total adversarial test Accuarcy: 12.24\n","Total benign test loss: 1.099336484814905\n","Total adversarial test loss: 3.687498604416088\n","**Summary**\n","Total validation accuracy: 36.73499999999999\n","Total validiation loss: 2.3934175446154966\n","Model at epoch7 saved\n","Epoch 9\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 72.1%, Avg loss: 1.674739 \n","\n","\n","Total benign test accuarcy: 60.18\n","Total adversarial test Accuarcy: 13.569999999999999\n","Total benign test loss: 1.1339335517518838\n","Total adversarial test loss: 3.7094168754140284\n","**Summary**\n","Total validation accuracy: 36.875\n","Total validiation loss: 2.4216752135829562\n","Model at epoch8 saved\n","Epoch 10\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 73.7%, Avg loss: 1.582002 \n","\n","\n","Total benign test accuarcy: 58.32000000000001\n","Total adversarial test Accuarcy: 13.950000000000001\n","Total benign test loss: 1.215383777193203\n","Total adversarial test loss: 3.7174345232119226\n","**Summary**\n","Total validation accuracy: 36.135000000000005\n","Total validiation loss: 2.466409150202563\n","Epoch 11\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 75.0%, Avg loss: 1.511218 \n","\n","\n","Total benign test accuarcy: 63.160000000000004\n","Total adversarial test Accuarcy: 15.120000000000001\n","Total benign test loss: 1.0362437678750154\n","Total adversarial test loss: 3.395193203239684\n","**Summary**\n","Total validation accuracy: 39.14\n","Total validiation loss: 2.2157184855573497\n","Model at epoch10 saved\n","Epoch 12\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 76.3%, Avg loss: 1.438355 \n","\n","\n","Total benign test accuarcy: 64.13\n","Total adversarial test Accuarcy: 14.91\n","Total benign test loss: 1.0332943394685248\n","Total adversarial test loss: 3.5001269647270252\n","**Summary**\n","Total validation accuracy: 39.519999999999996\n","Total validiation loss: 2.266710652097775\n","Model at epoch11 saved\n","Epoch 13\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 77.4%, Avg loss: 1.375416 \n","\n","\n","Total benign test accuarcy: 65.42999999999999\n","Total adversarial test Accuarcy: 15.47\n","Total benign test loss: 0.9974642923683118\n","Total adversarial test loss: 3.593797574377364\n","**Summary**\n","Total validation accuracy: 40.449999999999996\n","Total validiation loss: 2.2956309333728377\n","Model at epoch12 saved\n","Epoch 14\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 78.2%, Avg loss: 1.334259 \n","\n","\n","Total benign test accuarcy: 62.4\n","Total adversarial test Accuarcy: 14.530000000000001\n","Total benign test loss: 1.0851404340403854\n","Total adversarial test loss: 3.4636264211812597\n","**Summary**\n","Total validation accuracy: 38.464999999999996\n","Total validiation loss: 2.2743834276108226\n","Epoch 15\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 78.9%, Avg loss: 1.285058 \n","\n","\n","Total benign test accuarcy: 64.49000000000001\n","Total adversarial test Accuarcy: 15.559999999999999\n","Total benign test loss: 1.0322952164206536\n","Total adversarial test loss: 3.7334903152125656\n","**Summary**\n","Total validation accuracy: 40.025\n","Total validiation loss: 2.3828927658166097\n","Epoch 16\n","-------------------------------\n","batch no. 0\n","batch no. 20\n","batch no. 40\n","batch no. 60\n","batch no. 80\n","batch no. 100\n","batch no. 120\n","batch no. 140\n","batch no. 160\n","batch no. 180\n","batch no. 200\n","batch no. 220\n","batch no. 240\n","batch no. 260\n","batch no. 280\n","batch no. 300\n","batch no. 320\n","batch no. 340\n","batch no. 360\n","batch no. 380\n","batch no. 400\n","batch no. 420\n","batch no. 440\n","batch no. 460\n","batch no. 480\n","batch no. 500\n","batch no. 520\n","batch no. 540\n","batch no. 560\n","batch no. 580\n","batch no. 600\n","batch no. 620\n","ALP Train: \n"," Accuracy: 80.2%, Avg loss: 1.219085 \n","\n","\n","Total benign test accuarcy: 65.58\n","Total adversarial test Accuarcy: 15.770000000000001\n","Total benign test loss: 1.0139247759891923\n","Total adversarial test loss: 3.5446727063245835\n","**Summary**\n","Total validation accuracy: 40.675000000000004\n","Total validiation loss: 2.2792987411568877\n","Model at epoch15 saved\n","Done!\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7d3245a0f54667bb4d964271751527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>adv_acc</td><td>▃▁▃▁▃▄▅▅▆▆▇▇█▇██</td></tr><tr><td>adv_loss</td><td>▁▅▅█▅▅▃▆▆▆▄▅▅▄▇▅</td></tr><tr><td>benign_acc</td><td>▁▂▄▅▅▅▆▇▆▆▇██▇██</td></tr><tr><td>benign_loss</td><td>█▇▅▄▄▅▃▂▂▃▁▁▁▂▁▁</td></tr><tr><td>total_acc</td><td>▁▁▃▄▄▄▅▆▆▆▇▇█▇██</td></tr><tr><td>total_loss</td><td>▃█▆█▅▅▂▄▅▆▁▂▂▂▄▂</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>adv_acc</td><td>0.1577</td></tr><tr><td>adv_loss</td><td>3.54467</td></tr><tr><td>benign_acc</td><td>0.6558</td></tr><tr><td>benign_loss</td><td>1.01392</td></tr><tr><td>total_acc</td><td>0.40675</td></tr><tr><td>total_loss</td><td>2.2793</td></tr><tr><td>train_acc</td><td>0.80227</td></tr><tr><td>train_loss</td><td>1.21909</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">experiment_first</strong> at: <a href='https://wandb.ai/dayoonlee/First%20Trials/runs/etgnz4uc' target=\"_blank\">https://wandb.ai/dayoonlee/First%20Trials/runs/etgnz4uc</a><br/> View project at: <a href='https://wandb.ai/dayoonlee/First%20Trials' target=\"_blank\">https://wandb.ai/dayoonlee/First%20Trials</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240607_140137-etgnz4uc/logs</code>"]},"metadata":{}}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"50295d36f123455bbdd1d0abfc6149e2":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_3dc4575271324d528878828d2993eb22","IPY_MODEL_3aeda55ac16d44e3a5231ea3e930992d"],"layout":"IPY_MODEL_137b9d438c27461e8be17b8835f9e252"}},"3dc4575271324d528878828d2993eb22":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b85a0cb9bd304557b73fb538ff953b2a","placeholder":"​","style":"IPY_MODEL_1d20e5bee8cd4ad8b8fe2c55a6163591","value":"0.025 MB of 0.025 MB uploaded\r"}},"3aeda55ac16d44e3a5231ea3e930992d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e52bd17b64234292b090d806e6df29bf","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51d7de4c80d94c4980b53aab78633019","value":1}},"137b9d438c27461e8be17b8835f9e252":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b85a0cb9bd304557b73fb538ff953b2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d20e5bee8cd4ad8b8fe2c55a6163591":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e52bd17b64234292b090d806e6df29bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51d7de4c80d94c4980b53aab78633019":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec7d3245a0f54667bb4d964271751527":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_25c07fc9d34240588b693464e5f0a030","IPY_MODEL_6ef1db049f804ae3bbd934771699e08c"],"layout":"IPY_MODEL_bd61ca97f43440e5adbc6c17c521fafd"}},"25c07fc9d34240588b693464e5f0a030":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b2c63e5dd7e4cd080cc2ea131601ed7","placeholder":"​","style":"IPY_MODEL_9f2b4799b55240078f0efe19570e5fa1","value":"0.025 MB of 0.025 MB uploaded\r"}},"6ef1db049f804ae3bbd934771699e08c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd67e7a981f440c7b8eb6792c05691a6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b2565744faf42cbbff83e3cf8a81dcf","value":1}},"bd61ca97f43440e5adbc6c17c521fafd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b2c63e5dd7e4cd080cc2ea131601ed7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f2b4799b55240078f0efe19570e5fa1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd67e7a981f440c7b8eb6792c05691a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b2565744faf42cbbff83e3cf8a81dcf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}