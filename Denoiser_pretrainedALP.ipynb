{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torchvision\n","import os\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torchvision.transforms import ToTensor\n","from torch.utils.data.dataset import random_split\n","from torchvision import models\n","from torch.utils.data import Dataset\n","import torch.optim as optim"],"metadata":{"id":"FR3R1bdd_NfH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iuLZfVC8_pBj","executionInfo":{"status":"ok","timestamp":1721820224011,"user_tz":-540,"elapsed":37735,"user":{"displayName":"khloe Lee","userId":"00481896776510683202"}},"outputId":"1ef57f2d-286f-4f11-c59a-c8f2a8510bdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd '/content/drive/MyDrive/AI_project'"],"metadata":{"id":"h4T6wdH5xHsr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721820278214,"user_tz":-540,"elapsed":357,"user":{"displayName":"khloe Lee","userId":"00481896776510683202"}},"outputId":"75c02b60-d2cf-435e-c8bb-0ef32602ab24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AI_project\n"]}]},{"cell_type":"code","source":["from attacks.torchattacks import PGD\n","from Models import *"],"metadata":{"id":"YGeLKHyqfHV7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Dataset Preparation"],"metadata":{"id":"M5aE2di45kT0"}},{"cell_type":"code","source":["train_transform = transforms.Compose([\n","        transforms.RandomCrop(32,padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","test_transform =  transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform = train_transform)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","\n","                                       download=True, transform = test_transform)"],"metadata":{"id":"g-mL_jMg_JZd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721820320163,"user_tz":-540,"elapsed":12757,"user":{"displayName":"khloe Lee","userId":"00481896776510683202"}},"outputId":"fdbb450b-1e87-42cd-fa7b-8e3273b75d62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["# 2. Prepare Network"],"metadata":{"id":"ca2oALE09QHj"}},{"cell_type":"code","source":["class DenoiseLoss(nn.Module):\n","    def __init__(self, n=2, hard_mining = 0, norm = False):\n","        super(DenoiseLoss, self).__init__()\n","        self.n = n\n","        assert(hard_mining >= 0 and hard_mining <= 1)\n","        self.hard_mining = hard_mining\n","        self.norm = norm\n","\n","    def forward(self, x, y):\n","        loss = torch.pow(torch.abs(x - y), self.n) / self.n\n","        if self.hard_mining > 0:\n","            loss = loss.view(-1)\n","            k = int(loss.size(0) * self.hard_mining)\n","            loss, idcs = torch.topk(loss, k)\n","            y = y.view(-1)[idcs]\n","\n","        loss = loss.mean()\n","        if self.norm:\n","            norm = torch.pow(torch.abs(y), self.n)\n","            norm = norm.data.mean()\n","            loss = loss / norm\n","        return loss"],"metadata":{"id":"gtSonENcxiwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load ALP-pretrained model\n","class Net(nn.Module):\n","    def __init__(self, pre_res, n=2, hard_mining = 0, loss_norm = False):\n","        super(Net, self).__init__()\n","        self.denoiser = Denoise()\n","        self.net = pre_res\n","        for param in self.net.parameters():\n","            param.requires_grad = False\n","\n","        self.loss = DenoiseLoss(n, hard_mining, loss_norm)\n","\n","    def forward(self, orig_x, adv_x, requires_control = True, train = True):\n","        orig_outputs = self.net(orig_x)\n","\n","        if requires_control:\n","            control_outputs = self.net(adv_x)\n","            control_loss = self.loss(control_outputs, orig_outputs)\n","\n","        if train:\n","            adv_x.volatile = False\n","            for i in range(len(orig_outputs)):\n","                orig_outputs[i].volatile = False\n","        adv_x = self.denoiser(adv_x)\n","        adv_outputs = self.net(adv_x) #adv 할때는 resnet에서 param으로 받은 denoiser에 먼저 통과시켜서 denoising함\n","        loss = self.loss(adv_outputs, orig_outputs) #adv_outputs(net), original output\n","        return orig_outputs, adv_outputs, loss"],"metadata":{"id":"s6eTl_jp442k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","def train(dataloader,net, attack,optimizer): #adv+benign 1:1\n","    criterion =  nn.CrossEntropyLoss()\n","    net.train()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    train_loss, train_acc = 0, 0\n","    for batch, (X, y) in enumerate(dataloader):\n","        if(batch%20==0):\n","          print(f'batch no. {batch}')\n","        X = X.to(device)\n","        y = y.to(device)\n","        optimizer.zero_grad()\n","        adv_x = attack(X,y)\n","        orig_outputs,adv_outputs,loss = net(X, adv_x, requires_control = True, train = True)\n","        loss = loss+criterion(adv_outputs,y) #clean 과 denoised의 logit간 차이+denoised CE loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        train_acc += (orig_outputs.argmax(1) == y).type(torch.float).sum().item()\n","        train_acc += (adv_outputs.argmax(1) == y).type(torch.float).sum().item()\n","\n","    train_loss /= num_batches\n","    train_acc /= (2*size)\n","    print(f\"HGD Train: \\n Accuracy: {(100*train_acc):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n","\n","    return train_loss, train_acc\n","\n","def validation_loop(dataloader, net, attack):\n","    loss_fn = nn.CrossEntropyLoss()\n","    net.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    total_loss,total_acc = 0, 0\n","    benign_loss, benign_acc = 0, 0\n","    adv_loss, adv_acc = 0, 0\n","    for batch,(X,y) in enumerate(dataloader):\n","      with torch.no_grad():\n","        X= X.to(device)\n","        y = y.to(device)\n","        _,denoised_clean,_ = net(X, X, requires_control = False, train = False)\n","        benign_loss += loss_fn(denoised_clean, y).item()\n","        benign_acc += (denoised_clean.argmax(1) == y).type(torch.float).sum().item()\n","      X.requires_grad = True\n","      adv = attack(X,y).to(device)\n","      X.requires_grad = False\n","      with torch.no_grad():\n","        _,denoised_adv,_ = net(X, adv, requires_control = False, train = False)\n","        adv_loss += loss_fn(denoised_adv, y).item()\n","        adv_acc += (denoised_adv.argmax(1) == y).type(torch.float).sum().item()\n","\n","    # Compute total loss and accuracy\n","    benign_loss /= num_batches\n","    benign_acc /= size\n","    adv_loss /= num_batches\n","    adv_acc /= size\n","    total_loss = (benign_loss + adv_loss) / 2\n","    total_acc = (benign_acc + adv_acc) / 2\n","    print('\\nTotal benign test accuarcy:', 100.*benign_acc)\n","    print('Total adversarial test Accuarcy:', 100.*adv_acc)\n","    print('Total benign test loss:', benign_loss)\n","    print('Total adversarial test loss:', adv_loss)\n","    print('**Summary**')\n","    print('Total validation accuracy:',100*total_acc)\n","    print('Total validiation loss:',total_loss)\n","\n","    return total_loss,total_acc,benign_loss,benign_acc,adv_loss,adv_acc"],"metadata":{"id":"sEmKNGm96vfS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run(net,optimizer,model_save_name,epochs):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","   # Move model to device\n","    net = net.to(device)\n","    attack = PGD(net.net, eps=8/255,alpha=2/255,steps=7)\n","    best_validation_acc = 0.0\n","    for t in range(epochs):\n","        print(f\"Epoch {t+1}\\n-------------------------------\")\n","        train_loss, train_acc = train(trainloader, net, attack, optimizer)\n","        total_loss,total_acc,benign_loss,benign_acc,adv_loss,adv_acc = validation_loop(valloader, net, attack)\n","\n","\n","        if total_acc > best_validation_acc:\n","            best_validation_acc = total_acc\n","            print(f\"Net at epoch {t+1} saved\")\n","            path = F\"/content/drive/MyDrive/AI_project/SaveModel/best_{model_save_name}\"\n","            torch.save(net.state_dict(), path)\n","        path = F\"/content/drive/MyDrive/AI_project/SaveModel/last_{model_save_name}\"\n","        torch.save(net.state_dict(), path)\n","\n","    print(\"Done!\")"],"metadata":{"id":"0WkzcRYTVn34"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Hyperparameters\n","batch_size = 64\n","epochs = 10"],"metadata":{"id":"M3CLqpJ9gARC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","train_len = len(trainset)\n","num_train = int(train_len * 0.8)\n","num_val = train_len - num_train\n","train_set,val_set = random_split(trainset,[num_train, num_val])\n","\n","trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n","                                              shuffle=True, num_workers=2)\n","valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n","                                            shuffle=False, num_workers=2)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n","print(len(train_set))\n","print(len(val_set))"],"metadata":{"id":"E6_hTN9ffh0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pre_alp = ResNet50()\n","PATH = '/content/drive/MyDrive/AI_project/SaveModel/best_alp2.pt'\n","pre_alp.load_state_dict(torch.load(PATH))\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","net = Net(pre_alp)\n","optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)\n","run(net,optimizer,model_save_name=\"HGDnet_pre\",epochs=epochs)"],"metadata":{"id":"h7zQVui7BBpT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Testing on black box attack\n","def test_loop(dataloader, net, attack):\n","    loss_fn = nn.CrossEntropyLoss()\n","    net.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    total_loss,total_acc = 0, 0\n","    benign_loss, benign_acc = 0, 0\n","    adv_loss, adv_acc = 0, 0\n","    for batch,(X,y) in enumerate(dataloader):\n","      with torch.no_grad():\n","        X= X.to(device)\n","        y = y.to(device)\n","        _,denoised_clean,_ = net(X, X, requires_control = False, train = False)\n","        benign_loss += loss_fn(denoised_clean, y).item()\n","        benign_acc += (denoised_clean.argmax(1) == y).type(torch.float).sum().item()\n","      X.requires_grad = True\n","      adv = attack(X,y).to(device)\n","      X.requires_grad = False\n","      with torch.no_grad():\n","        _,denoised_adv,_ = net(X, adv, requires_control = False, train = False)\n","        adv_loss += loss_fn(denoised_adv, y).item()\n","        adv_acc += (denoised_adv.argmax(1) == y).type(torch.float).sum().item()\n","\n","    # Compute total loss and accuracy\n","    benign_loss /= num_batches\n","    benign_acc /= size\n","    adv_loss /= num_batches\n","    adv_acc /= size\n","    total_loss = (benign_loss + adv_loss) / 2\n","    total_acc = (benign_acc + adv_acc) / 2\n","    print('\\nTotal benign test accuarcy:', 100.*benign_acc)\n","    print('Total adversarial test Accuarcy:', 100.*adv_acc)\n","    print('Total benign test loss:', benign_loss)\n","    print('Total adversarial test loss:', adv_loss)\n","    print('**Summary**')\n","    print('Total Test accuracy:',100*total_acc)\n","    print('Total validation:',total_loss)\n","\n","    return total_loss,total_acc,benign_loss,benign_acc,adv_loss,adv_acc"],"metadata":{"id":"gcu3r5qEafb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PATH = '/content/drive/MyDrive/AI_project/SaveModel/best_HGDnet_pre.pt'\n","loaded_model = Net([32,32], Bottleneck, [64, 128, 256], [2, 2, 2], [128, 64], [2, 2], 2)\n","loaded_model.load_state_dict(torch.load(PATH))\n","loaded_model.to(device)\n","loaded_model.eval()\n","attack = PGD(loaded_model.net, eps=8/255,alpha=2/255,steps=7)"],"metadata":{"id":"WGTPI1MHayJk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False,num_workers=2)\n","total_loss,total_acc,benign_loss,benign_acc,adv_loss,adv_acc = test_loop(testloader,loaded_model,attack)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QisXe9USc4io","executionInfo":{"status":"ok","timestamp":1718896158056,"user_tz":-540,"elapsed":97523,"user":{"displayName":"lee ddd","userId":"00904851135334932184"}},"outputId":"49f7efa3-e313-4ca4-ff99-0a6d427d268f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Total benign test accuarcy: 30.7\n","Total adversarial test Accuarcy: 60.85\n","Total benign test loss: 2.5377121412070696\n","Total adversarial test loss: 1.2959480801965022\n","**Summary**\n","Total Test accuracy: 45.775\n","Total validation: 1.916830110701786\n"]}]}]}